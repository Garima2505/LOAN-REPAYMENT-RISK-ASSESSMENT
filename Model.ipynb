{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PredictDefaulter').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data. \n",
    "Reading the data that is cleaned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema of the data\n",
    "data_schema = StructType([\n",
    "    StructField('loan_amnt', DoubleType(), True),\n",
    "    StructField('funded_amnt', DoubleType(), True),\n",
    "    StructField('funded_amnt_inv', DoubleType(), True),\n",
    "    StructField('term', StringType(), True),\n",
    "    StructField('int_rate', FloatType(), True),\n",
    "    StructField('installment', DoubleType(), True),\n",
    "    StructField('grade', StringType(), True),\n",
    "    StructField('emp_length', StringType(), True),\n",
    "    StructField('home_ownership', StringType(), True),\n",
    "    StructField('annual_inc', DoubleType(), True),\n",
    "    StructField('verification_status', StringType(), True),\n",
    "    StructField('loan_status', StringType(), True),\n",
    "    StructField('addr_state', StringType(), True),\n",
    "    StructField('dti', FloatType(), True),\n",
    "    StructField('delinq_2yrs', DoubleType(), True),\n",
    "    StructField('fico_range_low', DoubleType(), True),\n",
    "    StructField('fico_range_high', DoubleType(), True),\n",
    "    StructField('pub_rec', DoubleType(), True),\n",
    "    StructField('revol_bal', DoubleType(), True),\n",
    "    StructField('revol_util', DoubleType(), True),\n",
    "    StructField('initial_list_status', StringType(), True),\n",
    "    StructField('total_pymnt', DoubleType(), True),\n",
    "    StructField('total_pymnt_inv', DoubleType(), True),\n",
    "    StructField('total_rec_prncp', DoubleType(), True),\n",
    "    StructField('total_rec_int', DoubleType(), True),\n",
    "    StructField('application_type', StringType(), True),\n",
    "    StructField('acc_now_delinq', DoubleType(), True),\n",
    "    StructField('tot_cur_bal', DoubleType(), True),\n",
    "    StructField('total_rev_hi_lim', DoubleType(), True),\n",
    "    StructField('avg_cur_bal', DoubleType(), True),\n",
    "    StructField('bc_open_to_buy', DoubleType(), True),\n",
    "    StructField('bc_util', DoubleType(), True),\n",
    "    StructField('chargeoff_within_12_mths', DoubleType(), True),\n",
    "    StructField('delinq_amnt', DoubleType(), True),\n",
    "    StructField('mort_acc', DoubleType(), True),\n",
    "    StructField('num_accts_ever_120_pd', DoubleType(), True),\n",
    "    StructField('num_actv_bc_tl', DoubleType(), True),\n",
    "    StructField('num_actv_rev_tl', DoubleType(), True),\n",
    "    StructField('num_bc_sats', DoubleType(), True),\n",
    "    StructField('num_bc_tl', DoubleType(), True),\n",
    "    StructField('num_il_tl', DoubleType(), True),\n",
    "    StructField('num_op_rev_tl', DoubleType(), True),\n",
    "    StructField('num_rev_accts', DoubleType(), True),\n",
    "    StructField('num_rev_tl_bal_gt_0', DoubleType(), True),\n",
    "    StructField('num_sats', DoubleType(), True),\n",
    "    StructField('pct_tl_nvr_dlq', DoubleType(), True),\n",
    "    StructField('pub_rec_bankruptcies', DoubleType(), True),\n",
    "    StructField('tax_liens', DoubleType(), True),\n",
    "    StructField('tot_hi_cred_lim', DoubleType(), True),\n",
    "    StructField('total_bal_ex_mort', DoubleType(), True),\n",
    "    StructField('total_bc_limit', DoubleType(), True),\n",
    "    StructField('total_il_high_credit_limit', DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('Data/CleanedDataOutput', header = True, schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_amnt: double (nullable = true)\n",
      " |-- funded_amnt: double (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- delinq_2yrs: double (nullable = true)\n",
      " |-- fico_range_low: double (nullable = true)\n",
      " |-- fico_range_high: double (nullable = true)\n",
      " |-- pub_rec: double (nullable = true)\n",
      " |-- revol_bal: double (nullable = true)\n",
      " |-- revol_util: double (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- total_pymnt: double (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- acc_now_delinq: double (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- total_rev_hi_lim: double (nullable = true)\n",
      " |-- avg_cur_bal: double (nullable = true)\n",
      " |-- bc_open_to_buy: double (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: double (nullable = true)\n",
      " |-- mort_acc: double (nullable = true)\n",
      " |-- num_accts_ever_120_pd: double (nullable = true)\n",
      " |-- num_actv_bc_tl: double (nullable = true)\n",
      " |-- num_actv_rev_tl: double (nullable = true)\n",
      " |-- num_bc_sats: double (nullable = true)\n",
      " |-- num_bc_tl: double (nullable = true)\n",
      " |-- num_il_tl: double (nullable = true)\n",
      " |-- num_op_rev_tl: double (nullable = true)\n",
      " |-- num_rev_accts: double (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: double (nullable = true)\n",
      " |-- num_sats: double (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: double (nullable = true)\n",
      " |-- tax_liens: double (nullable = true)\n",
      " |-- tot_hi_cred_lim: double (nullable = true)\n",
      " |-- total_bal_ex_mort: double (nullable = true)\n",
      " |-- total_bc_limit: double (nullable = true)\n",
      " |-- total_il_high_credit_limit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loan_amnt': 0,\n",
       " 'funded_amnt': 0,\n",
       " 'funded_amnt_inv': 0,\n",
       " 'term': 0,\n",
       " 'int_rate': 0,\n",
       " 'installment': 0,\n",
       " 'grade': 0,\n",
       " 'emp_length': 0,\n",
       " 'home_ownership': 0,\n",
       " 'annual_inc': 0,\n",
       " 'verification_status': 0,\n",
       " 'loan_status': 0,\n",
       " 'pymnt_plan': 0,\n",
       " 'addr_state': 0,\n",
       " 'dti': 0,\n",
       " 'delinq_2yrs': 0,\n",
       " 'fico_range_low': 0,\n",
       " 'fico_range_high': 0,\n",
       " 'pub_rec': 0,\n",
       " 'revol_bal': 0,\n",
       " 'revol_util': 0,\n",
       " 'initial_list_status': 0,\n",
       " 'total_pymnt': 0,\n",
       " 'total_pymnt_inv': 0,\n",
       " 'total_rec_prncp': 0,\n",
       " 'total_rec_int': 0,\n",
       " 'application_type': 0,\n",
       " 'acc_now_delinq': 0,\n",
       " 'tot_cur_bal': 0,\n",
       " 'total_rev_hi_lim': 0,\n",
       " 'avg_cur_bal': 0,\n",
       " 'bc_open_to_buy': 0,\n",
       " 'bc_util': 0,\n",
       " 'chargeoff_within_12_mths': 0,\n",
       " 'delinq_amnt': 0,\n",
       " 'mort_acc': 0,\n",
       " 'num_accts_ever_120_pd': 0,\n",
       " 'num_actv_bc_tl': 0,\n",
       " 'num_actv_rev_tl': 0,\n",
       " 'num_bc_sats': 0,\n",
       " 'num_bc_tl': 0,\n",
       " 'num_il_tl': 0,\n",
       " 'num_op_rev_tl': 0,\n",
       " 'num_rev_accts': 0,\n",
       " 'num_rev_tl_bal_gt_0': 0,\n",
       " 'num_sats': 0,\n",
       " 'pct_tl_nvr_dlq': 0,\n",
       " 'pub_rec_bankruptcies': 0,\n",
       " 'tax_liens': 0,\n",
       " 'tot_hi_cred_lim': 0,\n",
       " 'total_bal_ex_mort': 0,\n",
       " 'total_bc_limit': 0,\n",
       " 'total_il_high_credit_limit': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values\n",
    "data_null = data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in data.columns[:]])\n",
    "null_values = data_null.head(1)[0]\n",
    "null_values.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we will be only considering applicants that have taken loan Individually (where application_type = 'Individual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2215254"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_individual = data.filter(data['application_type'] == 'Individual').drop('application_type')\n",
    "data_individual.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Top 10 states with the maximum applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|addr_state| count|\n",
      "+----------+------+\n",
      "|        CA|310445|\n",
      "|        TX|185967|\n",
      "|        NY|183661|\n",
      "|        FL|157411|\n",
      "|        IL| 90197|\n",
      "|        NJ| 83029|\n",
      "|        PA| 74536|\n",
      "|        GA| 73234|\n",
      "|        OH| 72791|\n",
      "|        VA| 61793|\n",
      "+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_individual.groupBy('addr_state').count().orderBy(desc('count')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, maximum applicants are from California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|loan_status       |count  |\n",
      "+------------------+-------+\n",
      "|Fully Paid        |980714 |\n",
      "|Default           |36     |\n",
      "|In Grace Period   |10313  |\n",
      "|Charged Off       |238633 |\n",
      "|Late (31-120 days)|23993  |\n",
      "|Current           |1104088|\n",
      "|Late (16-30 days) |4856   |\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('loan_status').count().show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaning of different loan status:\n",
    "* Current: Loan is up to date on all outstanding payments. \n",
    "* In Grace Period: Loan is past due but within the 15-day grace period.  \n",
    "* Late (16-30): Loan has not been current for 16 to 30 days.\n",
    "* Late (31-120): Loan has not been current for 31 to 120 days.\n",
    "* Fully paid: Loan has been fully repaid, either at the expiration of the 3- or 5-year year term or as a result of a prepayment.\n",
    "* Default: Loan has not been current for an extended period of time.\n",
    "* Charged Off: Loan for which there is no longer a reasonable expectation of further payments. Upon Charge Off, the remaining principal balance of the Note is deducted from the account balance.\n",
    "\n",
    "### Difference between \"Charged Off\" and \"Default\"\n",
    "Loans that are in \"Default\" are loans for which borrowers have failed to make payments for an extended period of time.\n",
    " \n",
    "A loan becomes “Charged Off” when there is no longer a reasonable expectation of further payments.  Charge Off typically occurs when a loan is 120 days or more past due and there is no reasonable expectation of sufficient payment to prevent the charge off.  In certain circumstances, loans may be charged off at an earlier or later date.\n",
    " \n",
    "A loan that is in “Default” will still appear in your Notes, in the status of “Default,” while a loan that has been “Charged Off” will appear as charged off, and the remaining principal balance of the Note will be deducted from your account balance.\n",
    "\n",
    "Note: For simplicity, not considerating the applicants who \"Does not meet the credit policy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing how many applicants currently paying the loan might become defaulter, we are separeting out the applicants with \"loan_status\" as \"Current\" so that we can evaluate on it after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986618"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data with \"current\" status will later be use for evaluation purpose.\n",
    "df_current = data_individual.filter(data[\"loan_status\"] == \"Current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_current_applicants = df_current.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|loan_status       |\n",
      "+------------------+\n",
      "|Fully Paid        |\n",
      "|Default           |\n",
      "|In Grace Period   |\n",
      "|Charged Off       |\n",
      "|Late (31-120 days)|\n",
      "|Late (16-30 days) |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = data_individual.filter(data[\"loan_status\"] != 'Current')\n",
    "df.select('loan_status').distinct().show(truncate=False)\n",
    "total_applicant = df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To predict the risk whether an applicatant will pay the loan or not, we can consider \"loan_status\" in 3 categories: \n",
    "\n",
    "* safe : no risk in payment\n",
    "* low : low risk \n",
    "* high : high risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('default_risk', f.when(f.col('loan_status') == \"Fully Paid\", 'safe')\n",
    "                   .when(((f.col('loan_status') == \"In Grace Period\") | \n",
    "                                   (f.col('loan_status') == \"Late (31-120 days)\") |\n",
    "                         (f.col('loan_status') == 'Late (16-30 days)')) , \"low\")\n",
    "                   .otherwise(\"high\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|       loan_status|default_risk|\n",
      "+------------------+------------+\n",
      "|           Default|        high|\n",
      "|       Charged Off|        high|\n",
      "|   In Grace Period|         low|\n",
      "|Late (31-120 days)|         low|\n",
      "| Late (16-30 days)|         low|\n",
      "|        Fully Paid|        safe|\n",
      "+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('loan_status', 'default_risk').distinct().orderBy('default_risk').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|summary|       annual_inc|         loan_amnt|          int_rate|               dti|          mort_acc|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|  count|          1228636|           1228636|           1228636|           1228636|           1228636|\n",
      "|   mean| 78695.5284027327|14740.847777535413|13.303155751592813|18.213233423080506|1.6445106606024893|\n",
      "| stddev|53904.98488219384| 8770.525798898754| 4.816573433807678| 8.302813179558434|1.9840999102619745|\n",
      "|    min|           2000.0|            1000.0|              5.31|               1.0|               0.0|\n",
      "|    max|        4000000.0|           40000.0|             30.99|             49.96|              47.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['annual_inc', 'loan_amnt', 'int_rate', 'dti', 'mort_acc']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|emp_length| count|\n",
      "+----------+------+\n",
      "| 10+ years|435362|\n",
      "|   2 years|117840|\n",
      "|   3 years|104282|\n",
      "|  < 1 year|101352|\n",
      "|    1 year| 86050|\n",
      "|   5 years| 80768|\n",
      "|   4 years| 77105|\n",
      "|   6 years| 60118|\n",
      "|   8 years| 59046|\n",
      "|   7 years| 57346|\n",
      "|   9 years| 49367|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('emp_length').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Applicants that are 'safe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_safe = df.filter(df['default_risk'] == 'safe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_risk_safe = risk_safe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+\n",
      "|emp_length| count|        percentage|\n",
      "+----------+------+------------------+\n",
      "| 10+ years|344847| 35.89038273000331|\n",
      "|   2 years| 91502|  9.52318506630698|\n",
      "|   3 years| 80860| 8.415605609293593|\n",
      "|  < 1 year| 77718| 8.088598030461037|\n",
      "|    1 year| 66154|  6.88506027055662|\n",
      "|   5 years| 62902|6.5466043041774125|\n",
      "|   4 years| 59819| 6.225737224119879|\n",
      "|   6 years| 47124| 4.904489224985793|\n",
      "|   8 years| 46177|4.8059290158341605|\n",
      "|   7 years| 45097| 4.693526665376121|\n",
      "|   9 years| 38634| 4.020881858885093|\n",
      "+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "risk_safe.groupBy('emp_length').count().orderBy(desc('count')).withColumn('percentage', \n",
    "                                                                          (f.col('count') / total_risk_safe)*100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+\n",
      "|addr_state| count|        percentage|\n",
      "+----------+------+------------------+\n",
      "|        CA|140475|14.620111278326952|\n",
      "|        TX| 80019| 8.328077482686917|\n",
      "|        NY| 75810| 7.890020544651834|\n",
      "|        FL| 65768| 6.844886837892913|\n",
      "|        IL| 38042|  3.95926871863402|\n",
      "+----------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "risk_safe.groupBy('addr_state').count().orderBy(desc('count')).withColumn('percentage', \n",
    "                                                                          (f.col('count') / total_risk_safe)*100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     term| count|\n",
      "+---------+------+\n",
      "|36 months|761950|\n",
      "|60 months|198884|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# term\n",
    "risk_safe.groupBy('term').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|home_ownership| count|\n",
      "+--------------+------+\n",
      "|      MORTGAGE|493305|\n",
      "|          RENT|367241|\n",
      "|           OWN| 99754|\n",
      "|           ANY|   467|\n",
      "|          NONE|    34|\n",
      "|         OTHER|    33|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# home_ownership\n",
    "risk_safe.groupBy('home_ownership').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|verification_status| count|\n",
      "+-------------------+------+\n",
      "|    Source Verified|387522|\n",
      "|       Not Verified|313583|\n",
      "|           Verified|259729|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verification_status\n",
    "risk_safe.groupBy('verification_status').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicants that are on 'High' risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_high = df.filter(df['default_risk'] == 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_risk_high = risk_high.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|emp_length|count|        percentage|\n",
      "+----------+-----+------------------+\n",
      "| 10+ years|78987| 33.88502078480328|\n",
      "|   2 years|22774| 9.769930030930533|\n",
      "|   3 years|20313|  8.71417356275981|\n",
      "|  < 1 year|20076| 8.612501769603995|\n",
      "|    1 year|17250|  7.40016216007516|\n",
      "|   5 years|15452| 6.628829315796021|\n",
      "|   4 years|14835|6.3641394576646375|\n",
      "|   8 years|11519| 4.941592343298885|\n",
      "|   6 years|11341| 4.865231249705066|\n",
      "|   7 years|10909| 4.679905449522314|\n",
      "|   9 years| 9647| 4.138513875840294|\n",
      "+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "risk_high.groupBy('emp_length').count().orderBy(desc('count')).withColumn('percentage', \n",
    "                                                                          (f.col('count') / total_risk_high)*100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|addr_state|count|        percentage|\n",
      "+----------+-----+------------------+\n",
      "|        CA|33443| 14.34687670257354|\n",
      "|        NY|21244| 9.113567821949955|\n",
      "|        TX|19470| 8.352530855458745|\n",
      "|        FL|17449| 7.485532146733418|\n",
      "|        NJ| 9059|3.8862648700359927|\n",
      "+----------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "risk_high.groupBy('addr_state').count().orderBy(desc('count')).withColumn('percentage', \n",
    "                                                                          (f.col('count') / total_risk_high)*100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     term| count|\n",
      "+---------+------+\n",
      "|36 months|139029|\n",
      "|60 months| 94074|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# term\n",
    "risk_high.groupBy('term').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|home_ownership| count|\n",
      "+--------------+------+\n",
      "|          RENT|109042|\n",
      "|      MORTGAGE| 99357|\n",
      "|           OWN| 24590|\n",
      "|           ANY|   100|\n",
      "|         OTHER|     8|\n",
      "|          NONE|     6|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# home_ownership\n",
    "risk_high.groupBy('home_ownership').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|verification_status| count|\n",
      "+-------------------+------+\n",
      "|    Source Verified|101290|\n",
      "|           Verified| 79511|\n",
      "|       Not Verified| 52302|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verification_status\n",
    "risk_high.groupBy('verification_status').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing applicants with 'safe' and 'high' risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+------------------+\n",
      "|summary|       tot_cur_bal|              dti|   fico_range_low|   fico_range_high|\n",
      "+-------+------------------+-----------------+-----------------+------------------+\n",
      "|  count|            960834|           960834|           960834|            960834|\n",
      "|   mean|149146.79575035855|17.75943490759058|697.4491847707304| 701.4493127845185|\n",
      "| stddev|163503.35628624592|8.182468059114372| 32.1849966404692|32.185585498192346|\n",
      "|    min|               0.0|              1.0|            660.0|             664.0|\n",
      "|    max|         5445012.0|            49.96|            845.0|             850.0|\n",
      "+-------+------------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "risk_safe.describe(['tot_cur_bal', 'dti', 'fico_range_low', 'fico_range_high']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+\n",
      "|summary|       tot_cur_bal|              dti|   fico_range_low|  fico_range_high|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+\n",
      "|  count|            233103|           233103|           233103|           233103|\n",
      "|   mean|121889.50687035345|19.99958726399929|687.0949108334084|691.0949666027465|\n",
      "| stddev|136314.01115054343|8.500327836422429|25.07914407358612| 25.0794963232977|\n",
      "|    min|               0.0|              1.0|            660.0|            664.0|\n",
      "|    max|         3437283.0|            49.92|            845.0|            850.0|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "risk_high.describe(['tot_cur_bal', 'dti', 'fico_range_low', 'fico_range_high']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+--------------------+\n",
      "|summary|       delinq_2yrs|            pub_rec|       delinq_amnt|pub_rec_bankruptcies|\n",
      "+-------+------------------+-------------------+------------------+--------------------+\n",
      "|  count|            960834|             960834|            960834|              960834|\n",
      "|   mean| 0.319005155937446|0.20838771317417992|14.194593446942969| 0.12985489689165872|\n",
      "| stddev|0.8786005595481192| 0.5913702714030439| 773.6206934564974|  0.3713156286661652|\n",
      "|    min|                 0|                  0|               0.0|                 0.0|\n",
      "|    max|                39|                 63|          185408.0|                12.0|\n",
      "+-------+------------------+-------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "risk_safe.describe(['delinq_2yrs', 'pub_rec', 'delinq_amnt', 'pub_rec_bankruptcies']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------+--------------------+\n",
      "|summary|        delinq_2yrs|            pub_rec|       delinq_amnt|pub_rec_bankruptcies|\n",
      "+-------+-------------------+-------------------+------------------+--------------------+\n",
      "|  count|             233103|             233103|            233103|              233103|\n",
      "|   mean|0.36051445069347027|0.24633745597439757|20.694813022569424| 0.15174836874686298|\n",
      "| stddev| 0.9512137192283301| 0.6653008059429222|1026.5632144741928| 0.40478093739772136|\n",
      "|    min|                  0|                  0|               0.0|                 0.0|\n",
      "|    max|                 27|                 86|          249925.0|                11.0|\n",
      "+-------+-------------------+-------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "risk_high.describe(['delinq_2yrs', 'pub_rec', 'delinq_amnt', 'pub_rec_bankruptcies']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing 'default_risk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|default_risk|\n",
      "+-----+------------+\n",
      "|  1.0|        high|\n",
      "|  2.0|         low|\n",
      "|  0.0|        safe|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert loan_default into label indices using the StringIndexer\n",
    "label_stringIndex = StringIndexer(inputCol=\"default_risk\", outputCol=\"label\")\n",
    "df_label = label_stringIndex.fit(df).transform(df)\n",
    "df_label.select('label', 'default_risk').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_label.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861134"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------------+\n",
      "|default_risk| count|        percentage|\n",
      "+------------+------+------------------+\n",
      "|         low| 24268|1.9751985128223493|\n",
      "|        safe|673274| 54.79849198623514|\n",
      "|        high|163592|13.314928099127814|\n",
      "+------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy('default_risk').count().withColumn('percentage', \n",
    "                                                             (f.col('count')*100/total_applicant)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is skewed with 13% data of applicants at high risk, 54% safe and 2% applicants at lower risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = train_data.filter(train_data['default_risk'] == 'high')\n",
    "high_count = high.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = train_data.filter(train_data['default_risk'] == 'low')\n",
    "low_count = low.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe = train_data.filter(train_data['default_risk'] == 'safe')\n",
    "safe_count = safe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163592"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"low\" risk count after oversampling 145608\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, array, lit\n",
    "\n",
    "# oversample data from \"low\"\n",
    "oversample_ratio = int(high_count / low_count)\n",
    "low_range = range(oversample_ratio)\n",
    "oversampled_low = low.withColumn(\"dummy\", explode(array([lit(x) for x in low_range]))).drop('dummy')\n",
    "print(f'\"low\" risk count after oversampling {oversampled_low.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"safe\" count after undersampling 163256\n"
     ]
    }
   ],
   "source": [
    "# Undersampling data from \"safe\"\n",
    "\n",
    "undersample_ratio = float(high_count / safe_count)\n",
    "undersample_safe = safe.sample(withReplacement = False, fraction = undersample_ratio)\n",
    "print(f'\"safe\" count after undersampling {undersample_safe.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------------+\n",
      "|default_risk| count|        percentage|\n",
      "+------------+------+------------------+\n",
      "|         low|145608|11.851191076934096|\n",
      "|        safe|163256| 13.28758069924697|\n",
      "|        high|163592|13.314928099127814|\n",
      "+------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining all the dataframes\n",
    "\n",
    "final_train_df = oversampled_low.union(undersample_safe).union(high)\n",
    "final_train_df.groupBy('default_risk').count().withColumn('percentage', \n",
    "                                                             (f.col('count')*100/total_applicant)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['term', 'grade',\n",
    "                       'emp_length', 'home_ownership', \n",
    "                       'verification_status', 'pymnt_plan',\n",
    "                       'addr_state', 'initial_list_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns=['loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
    "                 'int_rate', 'installment', 'annual_inc',\n",
    "                 'dti', 'delinq_2yrs', 'fico_range_low',\n",
    "                 'fico_range_high', 'pub_rec', 'revol_bal',\n",
    "                 'revol_util', 'total_pymnt', 'total_pymnt_inv',\n",
    "                 'total_rec_prncp', 'total_rec_int', 'acc_now_delinq',\n",
    "                 'tot_cur_bal', 'total_rev_hi_lim', 'avg_cur_bal',\n",
    "                 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths',\n",
    "                 'delinq_amnt', 'mort_acc', 'num_accts_ever_120_pd',\n",
    "                 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats',\n",
    "                 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
    "                 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats',\n",
    "                 'pct_tl_nvr_dlq', 'pub_rec_bankruptcies', 'tax_liens',\n",
    "                 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit',\n",
    "                 'total_il_high_credit_limit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = final_train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categorical_columns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\").setHandleInvalid(\"skip\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = [c + \"classVec\" for c in categorical_columns] + numeric_columns\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_573c58cfc696,\n",
       " OneHotEncoder_a3743850440a,\n",
       " StringIndexer_23ae95f2c846,\n",
       " OneHotEncoder_29325e2e6dd7,\n",
       " StringIndexer_cd28ea4d8031,\n",
       " OneHotEncoder_dcb4436dbf9f,\n",
       " StringIndexer_911f5c0c923a,\n",
       " OneHotEncoder_4525982340ac,\n",
       " StringIndexer_024ac3796ba1,\n",
       " OneHotEncoder_6205d0d9e7d9,\n",
       " StringIndexer_3c52a808e2b7,\n",
       " OneHotEncoder_31f4a1fcd583,\n",
       " StringIndexer_42411f0745a8,\n",
       " OneHotEncoder_3254e27caaab,\n",
       " StringIndexer_d73d771672bd,\n",
       " OneHotEncoder_0e66d8c3649d,\n",
       " VectorAssembler_34b3d66b5f79]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = pipeline.fit(final_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing train and test data\n",
    "train_prep_df = pipelineModel.transform(final_train_df)\n",
    "test_prep_df = pipelineModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping relevant columns\n",
    "selected_columns = [\"features\"] + columns\n",
    "train_prep_df = train_prep_df.select(selected_columns)\n",
    "\n",
    "test_prep_df = test_prep_df.select(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|default_risk|\n",
      "+-----+------------+\n",
      "|  1.0|        high|\n",
      "|  2.0|         low|\n",
      "|  0.0|        safe|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_prep_df.select(\"label\", \"default_risk\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol='label')\n",
    "lr_model = lr.fit(train_prep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8414011040181519"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary = lr_model.summary\n",
    "training_summary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "lr_predictions = lr_model.transform(test_prep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|prediction|label| count|\n",
      "+----------+-----+------+\n",
      "|       2.0|  0.0|   768|\n",
      "|       1.0|  1.0| 53905|\n",
      "|       0.0|  1.0|   734|\n",
      "|       1.0|  0.0|    86|\n",
      "|       2.0|  2.0|  7614|\n",
      "|       2.0|  1.0| 14872|\n",
      "|       1.0|  2.0|  2551|\n",
      "|       0.0|  0.0|286706|\n",
      "|       0.0|  2.0|   266|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predictions.groupBy('prediction', 'label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475458636959799"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MulticlassClassificationEvaluator for calculating Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label',\n",
    "                                             metricName='accuracy')\n",
    "evaluator.evaluate(lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt_model = dt.fit(train_prep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "dt_predictions = dt_model.transform(test_prep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(118, {74: 0.0457, 75: 0.2275, 77: 0.0253, 79: 0.0165, 89: 0.0161, 90: 0.5827, 91: 0.0862})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122811303188962"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(dt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(numTrees=50)\n",
    "rf_model = rf.fit(train_prep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "rf_predictions = rf_model.transform(test_prep_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8859041275476554"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on different number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with number of trees 50 is: 0.8859041275476554\n",
      "Accuracy with number of trees 100 is: 0.8880158077448178\n",
      "Accuracy with number of trees 200 is: 0.8855757042779167\n",
      "Accuracy with number of trees 300 is: 0.8858552711934793\n",
      "Accuracy with number of trees 400 is: 0.8870848227735753\n"
     ]
    }
   ],
   "source": [
    "numTrees = [50, 100, 200, 300, 400]\n",
    "for num in numTrees:\n",
    "    rf = RandomForestClassifier(numTrees=num)\n",
    "    \n",
    "    # model\n",
    "    rf_model = rf.fit(train_prep_df)\n",
    "    \n",
    "    # Predict\n",
    "    rf_predictions = rf_model.transform(test_prep_df)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = evaluator.evaluate(rf_predictions)\n",
    "    print(f'Accuracy with number of trees {num} is: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to evalute whether the applicants, who are under current status, will face any financial distress in the future i.e. in the context of repaying the loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_current_df = pipelineModel.transform(df_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "lr_risk_prediction = lr_model.transform(predicted_current_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|       0.0| 30609|\n",
      "|       1.0|246527|\n",
      "|       2.0|709482|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_risk_prediction.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString \n",
    "\n",
    "converter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_label\", labels=['safe', 'high', 'low'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------+\n",
      "|prediction|predicted_label| count|\n",
      "+----------+---------------+------+\n",
      "|       1.0|           high|246527|\n",
      "|       2.0|            low|709482|\n",
      "|       0.0|           safe| 30609|\n",
      "+----------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_label_df = converter.transform(lr_risk_prediction)\n",
    "original_label_df.groupBy('prediction', \"predicted_label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicants_default = original_label_df.filter(original_label_df['predicted_label'] == 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of people that may default the loan : 24.98707706528768\n"
     ]
    }
   ],
   "source": [
    "total_applicants = converted.count()\n",
    "print(f\"percentage of people that may default the loan : {(applicants_default.count() / total_current_applicants)*100 }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
